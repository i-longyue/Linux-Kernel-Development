 第16章 页高速缓存和页回写
	页高速缓存是linux内核实现磁盘缓存.它主要用来减少磁盘的I/O操作.具体地讲,是把磁盘的数据缓存到物理
	内存中,把磁盘的访问变为对物理内存的访问.
	这一章将讨论页高速缓存与页回收写(将页高速缓存中变更数据刷新回磁盘的操作)

	磁盘高速缓存之所以在任何操作系统中重要的源自两个因素:
	第一,访问磁盘的速度要远远低于访问内存的速度----ms和ns的差距.
	因此,从内存访问数据比从磁盘访问速度更快,若从处理器的L1和L2高速缓存访问则更快.第二,数据一旦被
	访问,就很有可以在短期内再次被访问到.这种在短时期内集中访问同一数据的原理称作临时局部原理.

	临时局部原理能保证:如果在第一次访问数据时缓存它,那就有极有可能在短时期再次被高速缓存命中.
	正是由于内存缓存访问要比磁盘访问快得多,再加上数据一次被访问后更可能再次被访问的特点,所以磁盘
	的内存缓存将给系统存储性能带来质的飞跃.

16.1 缓存手段
	页高速缓存是由内存中的物理页面组成的,其内容对应磁盘上的物理块.页高速缓存大小能动态调整----它
	可以通过占用空闲内存以扩张大小,也可以自我收缩以缓解内存使用压力

	我们称被缓存的存储设备为后备存储,因为缓存背后的磁盘无疑才是所有缓存数据的归属.当内核开始一个读
	操作(比如,进程发起一个read()系统调用),它首先会检查需要的数据是否在页高缓存中.如果在,则放弃访问
	磁盘,而直接从内存中读取.这个行为称作缓存命中.如果数据没有在缓存中,称为缓存未命中.
	那么内核必须调度块I/O操作从磁盘去读取数据.然后将内存读来的数据放入缓存.缓存就可以持有某个文件
	的全部内容,也可以存储另一些文件的一页或者几页.到底该缓存谁取决于谁被访问到.

16.1.1 写缓存
	上面解释了在读操作过程中页高速缓存的作用,那么在进程写磁盘时,比如执行write()系统调用,缓存如何被
	使用呢?通常来说,缓存一般被实现成下面三种策略之一:第一种策略称为不缓存,也就是说高速缓存不去缓存
	任何写操作.当对一个缓存中数据进行写时,将直接跳过缓存写到磁盘,同时也使缓存中的数据失效.那么如果
	后续读操作进行时,需要再重新从磁盘中读取数据.不过这种策略很少使用,因为该策略不但不去缓存写操作
	而且需要额外费力去使缓存数据失效.

	第二种策略,写操作将自动更新内存缓存,同时也更新磁盘文件.这种方式,通常称为写透缓存,因为写操作会
	立刻穿透缓存缓存到磁盘中,这种策略对保持缓存一致性很有好处--缓存数据时刻和后备存储保持同步,
	所以不需要让缓存失效,同时它的实现也最简单.

	第三种策略,也是LInux.称为"回写",在这策略下程序执行操作直接写到缓存中,后端存储不会立即直接更新
	而是将页高速缓存中被写入的页面标记为"脏",并且被加入到脏页链表中,然后由一个进程周期行脏的页写
	回到磁盘,从而让磁盘中的数据和内存中最终一致.最后清理"脏"页标识.

16.1.2 缓存回收
	缓存算法最后涉及的重要内容是缓存中的数据如何清除,或者是为了更重要的缓存项腾出位置;或者是收缩
	缓存大小,腾出内存给其他地方使用.这个工作,也就是决定缓存中什么内容将清除的策略,称为缓存回收策略
	Linux的缓存回收是通过选择干净页进行简单替换.
	如果缓存中没有足够干净页面,内核将强制地进行回写操作,理想的回收策略是回收那些以后最不可以使用的页面
	当然要知道以后的事情你必须是告知,也正是这个原因,理想的回收策略称为算法.这种策略太理想了,无法
	真正实现.

	1.最近最少使用
	缓存回收策略通过所访问的数据特性,最成功的算法(特别是对于通用目的的页高速缓存)称为最近最少使用算法
	简称LRU.LRU回收策略需要跟踪每个页面的访问,以便能回收最老时间的页面

	2.双链策略
	Linux实现的是一个修改过的LRU,也称为双链策略.和以前不同,linux维护的不再是一个LUR链表,而是维护两个
	链表;活跃链表和非活跃链表.两个链表都被伪LRU规则维护,页面从尾部加入,从头部移除,如同队列.
	两个链表需要维持平衡----如果活跃链表变得过多而超过了非活跃链表,那么活跃链表的头页面将被重新称回
	到非活跃的链表中,以便能再被回收
	这种链表方式也称作LUR/2.更普通的是n个链表,故称LRU/n.

	我们现在知道页缓存如何构建,如何在写时被同步以及旧数据如何被回收来容纳新数据.现在让我们看看真实
	世界应用场景中,页高速缓存如何帮助系统.假定你在开发一个很大的软件工程(比如Linux内核)那么你将有大量
	的源文件被打开,只要你打开读取源文件.这些文件将被存储在页高速缓存中.只要数据被缓存.那么从一个文件
	跳到另一个文件将瞬间完成.当你编辑文件时,存储文件也会瞬间完成,因为写操作只需要写到内存.

	当你编译项目时,缓存的文件将使得编译过程更少访问磁盘,所以编译速度快了


16.2 Linux页高速缓存
	从名字可以看出,页高速缓存缓存的是内存页面.缓存中的页来自对正规文件,块设备文件和内存映射文件的读写
	如此一来,页高速缓存就包含了最近访问过的文件的数据块.在执行一个I/O操作前(比如read()操作),
	内核会检查数据是否已经在页高速缓存中了,如果所需要的数据确实在高速缓存中,那么内核可以从内存中
	迅速地返回需要的页,而不再见需要从相对较慢的磁盘上读取数据.在接下来的章节中,我们将剖析具体数据
	结构以及内核如何使用它们管理缓存.

16.2.1 address_space 对象
	在页高速缓存中的页可能包含了多个不连续的物理磁盘块.也正是由于页映射的磁盘块不一定连续,所以在页
	高速缓存中检查特定数据是否已经被缓存是件颇为困难的工作.因为不能用设备名称和块号来做页高速缓存中
	的数据的索引,这是简单的定位方法.

	Release 4引入页高速缓存时,仅仅只用作缓存文件系统数据,所以SVR4的页高速缓存使用它的等阶文件对象
	管理页高速缓存.Linux页高速缓的目标是缓存任何基于页的对象,这包含各种类型的文件和各种类型的内存
	映射

	address_space页总数由nrpages字段描述.
	address_space结构往往会和某些内核对象关联.通常情况下,它会与一个索引节点(inode)关联,这时
	host域就会指向该索引节点;如果关联对象不是一个索引节点的话,比如address_space swapper关联
	时,host域会被置为NULL.


16.2.2 address_space操作
	a_ops域指向地址空间对象中操作函数表,这与VFS对象及其操作表关系类似,操作函数表定义在文件<linux/fs.h
	由address_space_operations结构体表表示:
	struct address_space_operations{
		int (*)
			..
			..
			..
			..
	}
这些方法指针指向那些为指定缓存对象实现的页I/O操作.每个后备存储都通过自己的address_space_operation描述
自己如何与页缓存交互.比如ext3文件系统fs/ext3/inode.c中定义自己的操作表.这些方法提供了管理页高速缓存的
各种行为,包括最常用的读页到缓存,更新缓存数据.这里面readpage()和writepage()两个方法最为重要.我们下面
就来看看一个页面读操作会包含那些步骤.首先Linux内核试图在页高速缓存中找到需要的数据;
find_get_page()方法负责完成这个检查动作.一个address_space对象和一个偏移量会传给find_get_page()方法
用于高速缓存中搜索的数据:

	page = find_get_page(mapping, index);

	这里mapping是指定的地址空间,index是文件中指定位置,以页面为单位(是的,称address_space结构体为)
	mapping,又是一个容易混淆的命名,虽然我也在重复这种内核命名不一致的问题,但我们还是鄙视它.)
	如果搜索的页并没有高速缓存中,find_get_page()将会返回一个NULL,并且内核将分配一个新页面,然后将
	之前搜索的页加入页高速缓存中.

	首先,在页高速缓存中搜索需要的页.如果需要的页不在高速缓存中,那么内核在高速缓存中新分配一空闲项.
	下一步内核创建一个请求,接着数据被用户空间拷贝到内核缓冲;最后将数据写入磁盘.

	因为所有的页I/O操作都要执行上面这些步骤,这就保证了所有的页I/O操作必须都是通过页高速缓存进行的.
	因此,内核也总是试图先通过页高速缓存来满足所有的请求.
	如果在页高速缓存中未搜索到需要的页,则内核将从磁盘读入需要的页,然后将页加入高速缓存中,对于写
	所有被写的页都要加入页高速缓存中.


16.2.3 基树
	因为在任何页I/O操作前内核都要检查页是否已经在页高速缓存中了,所以这种频繁进行的检查必须迅速
	高效,否则搜索和检查页高速缓存的开销可能抵消页高速缓存带来的好处

	正如在16.2.2节所看到的,页高速缓存通过两个参数address_space对象加上一个偏移量进行搜索.
	每个address_space对象都有唯一的基树,它保存在page_tree结构体中,基树是一个二叉树,
	只要指定了文件偏移量,就可以在基树中迅速检索到希望的页.页高速缓存的搜索函数find_get_page()
	要调用的函数radix_tree_lookup(),该函数会在指定基树中搜索指定页面.
	
	基树核心代码的通用形式可以在文件lib/raidx-tree.c中找到.另外,要想使用基树,需要包含头文件.

16.2.4 以前的页散列表

	在2.6版本以前,内核页高速缓存不是通过基树检索,而是通过一个维护了系统中所有页的全局列表进行
	检索.对于给定的一个键值,该散列表会返回一个双向链表的入口对应于这个所给定的值.
	如果需要的页贮存在缓存中,那么链表中就会与其对应.否则,页就不在面高速缓存中,散列函数返回NULL
	全局散列表主要存在四个问题:

	*由于使用单个全局锁保护散列表,所以即使在中等规模的机器在中,锁的急用情况也会相当严重,造成性能
	受损.
	*由于散列表需要包含所有页高速缓存中的页,可是搜索需要的只是和当前文件相关的那些页,所以散列表
	包含所有页面相比搜索需要的页面要大得多.

	*如果散列搜索失败(也就是给定的页不在页高速缓存中),执行速度比希望的要慢得多,这是因为检索必须
	遍历指定散列键值对应的整个链表.

	*散列表比其他方法会消耗更多的内存.
	2.6版本内核中引入基于基树的页高速缓存来解决这些问题.

16.3 缓冲区高速缓存
	独立的磁盘块通过块I/O缓冲也要存入页高速缓存.
	一个缓存是一个物理磁盘块在内存里的表示.缓冲的作用就是映射内存中的页面到磁盘块.
	这样一来页高速缓存在块I/O操作时也减少了磁盘访问,因为它缓存磁盘块和减少块I/O操作.
	这个缓存通常称为缓冲区高速缓存,虽然实现上它没有作为独立缓存,而是作为页高速缓存的一部分.

	块I/O操作一次操作一个单独的磁盘块.普通的块I/O操作是读写i节点.内核提供了
	bread()函数实现从磁盘读一个块的底层操作.通过缓存,磁盘块映射到它们相关的内存页,并
	缓存到页高速缓存中.

	缓冲和页高速缓存并非天生就是统一的,2.4内核的主要工作之一就是统一它们.在更早的内核中.
	有两个独立的磁盘缓存,页高速缓存和缓冲区高速缓存.前者缓存页面,后者缓存缓冲区,这两个
	缓存并没有统一.一个磁盘块可以同时在于两个缓存中,这导致必须同步操作两个缓冲区数据,
	而且浪费了内存,去存储重复缓存项.




16.4 flusher线程
	由于页高速缓存的缓存作用,写操作实际会被延迟.当页高速缓存中的数据比后台存储的数据
	更新时,该数据就称作脏数据.在内存中累积起来的脏页最终必须被写回磁盘.在以下3种情况
	发生时,脏页被写回磁盘:
		*当空闲内存低于一个特定的阈值时,内核必须将脏页写回磁盘以便释放内存,因为只有
		干净内存才可以被回收.当内存干净后,内核就可以从缓存清理数据,然后收缩缓存,最终
		释放出更多的内存.

	*当脏页在内存中驻留时间超过一个特定阈值时,内核必须将超时的脏页写回磁盘,以
	保护脏页不会无限期地驻留在内存中.

	*当用户进程调用sync()和fsync()系统调用时,内核会按要求执行回写动作.
	上面三种工作的目的完全不同.实际上,在旧内核中,这是由两个独立的内核线程分别完成的.
	但是在2.6内核中,由一群内核线程(flusher线程)执行这三种工作.
	首先,flusher线程在系统中的空闲内存低于一个特定的阈值时,将脏页刷新写加磁盘.该后台
	回写例程的目的在于----在可用物理内存过低,释放脏页以重新获得内存.这个特定的内存阈值
	可以通过dirty_background_ratio sysctl系统调用设置.当空闲内存比阈值dirty_background_
	进程一步调用函数bdi_writeback_all()开始将脏页写回磁盘.该函数需要一个参数----试图写
	回的页面数目.函数连续地写出数据,直到满足以下两个条件:

	*已经有指定的最小数目的页被写出到磁盘.
	*空闲内存数已经回升,超过了阈值dirty_background_ratio.
	上述条件确保了flusher线程操作可以减轻系统中内存不足的压力.回写操作不会在
	达到这两个条件前停止,除非刷新线程写回了所有的脏页,没有剩下的脏页可再被写
	回了.

	为了满足第二个目标,flusher线程后台例程会被周期性唤醒,将那些内存中驻留时间过长的
	脏页写出,确保内存中不会有长期存在的脏页.如果系统发生崩溃,由于内存牌混乱之中,所以
	那些在内存中还没来得及写加磁盘的脏页就会丢失.所以周期性同步高级缓存和磁盘非常
	重要.在系统启动时,内核初始化一个定时器,让它周期地唤醒flusher线程的脏页写回,然后
	定时器将再次被初始化为dirty_expire_centisecs秒后唤醒flusher线程.
	flusher线程周期性地被唤醒并且把超过特定期限的脏页写加磁盘.

	系统管理员可以在/proc/sys/vm/中设置回写相关参数,也可以通过sysctl系统调用设置它们.
	flusher线程的实现代码在文件mm/page-writeback.c和mm/backing-dev.c中,回写机制的实现
	代码在文件fs/fs-writeback.c中.

16.4.1 膝上型计算机
	膝上型计算机模式是一种特殊的页回写策略,将硬盘转动的机制行为最少化,允许硬盘尽可能
	长时间地停滞,以此延长电池供电时间.该模式可通过/proc/sys/vm/laptop_mode文件进行配置
	通常,上述配置文件内容为0,也就是说膝上型计算机模式关闭,如果需要启用膝上计算机,则
	向配置文件中写入1.

	膝上型计算机模式的页回写行为与传统方式相比只有一片变化.除了当缓存中页面太旧时要
	执行回写脏页以外,flusher还会找准磁盘运转的时机,把所有其他的物理磁盘I/O,刷新脏
	缓冲等通通写回到磁盘,以便保证不会专门为了写磁盘而去主动激活磁盘运行.

	上述回写行为变化要求dirty_expire_interval和dirty_writeback_interval两阈值必须
	设置得更大.比如10分钟,因为磁盘运转并不很频繁.
	多数Linux发布版会在计算机接上电池或是拨掉电池时,自动开启禁止膝上型计算机模式以及
	其他需要的回写可调节开关.而在插上交流电源时恢复到常规的页回写模式.


16.4.2 历史上的bdflush kupdated pdflush
	在2.6版本前,flusher线程的工作是分别由bdflush和kupdate两个线程共同完成.
	当可用内存过低时,bdflush内核线程在后台执行页回写操作.类似flusher,它也有一组阈值参数
	当系统中空闲内存消耗到特定阈值以下时,bdflush线程就被wakeup_bdflush()函数唤醒

	在2.6内核中,buflush和Kupdated已让路给了pdflush线程----page dirty flush的缩写.Pdflush
	线程的执行和今天的flusher线程类似.其主要区别在于,pdflush线程数目动态的.默认是2个到8个
	具体多少取决于系统I/O的负载.
	Pdflush线程与任何任务都无关,它们是面向系统所有磁盘的全局任务.这样做的好处是实现简单
	可带来的问题是,pdflush线程很容易在拥塞的磁盘上绊住,而现代硬件发生拥塞更是家常便饭.
	采用每个磁盘一个刷新线程可以使得I/O操作同步执行.


16.4.3 避免拥塞的方法:使用多线程
	使用bdflush线程最主要的一个缺(rm)点就是,bdflush仅仅包含了一个线程,因此很有可能在页
	回写任务很重时,造成拥塞.这是因为单一的线程有可能堵塞在某个设备的已拥塞请求队列上,而
	其他设备的请求队列却没法得到处理.如果系统有多个磁盘和较强的处理能力,内核应该使得每个
	磁盘都处于忙状态.不幸的是,即使还有许多数据需要回写,单个的bdflush线程也可能会堵塞在
	某个队列的处理上.为了避免出现这种情况,内核需要多个回写线程并发执行,这样单个设备队列
	的拥塞就不会成为系统瓶颈.

	2.6内核通过使用多个flusher线程来解决上述问题.每个线程可以相互独立地将脏页刷新加磁盘,
	而且不同的flusher线程处理不同的设备队列.pdflush线程策略中,线程数是变动的
	
	每一个线程试图尽可能忙地从每个超级块的脏页链表中回收数据,并且写回到磁盘.
	pdflush方式避免了因为一个忙磁盘,使得其它磁盘饥饿的状况
	pdflush线程彩了拥塞回避策略:它们会主动尝试从那些没有拥塞的队列回写页.从而,pdflush线程
	将其工作调度开来,防止了仅仅欺负某个忙碌设备.
		
	这种方式效果确实不错,但是拥塞回避并不完美.
	因为使用pdflush以及后来的flusher线程提升了页回写性能.2.6内核系统相比早期内核可以让磁盘利
	更饱和.在系统I/O重的时候,fluhser 线路可以在每个磁盘上都维护更高的吞吐量.

16.5 小结
	本章中我们看到了Linux的页高速缓存和页回写.了解了内核如何通过页缓存执行页I/O操作以及
	这些页高速缓存(通过存储数据在内存中)可以利用减少磁盘I/O,从而极大地提升系统的性能.
	我们讨论了通过称为"回写缓存"的进程维护在缓存中的更新页面----具体的做法是标志内存中
	的页面为脏,然后找时机延迟写到磁盘中.Flusher内核中线程将负责处理这些最终的页
	回写操作.
	通过最近几章的学习,你应该已经对内存与文件系统有了深刻的认识,那么我们接下来进入模块专题.
	去学习Linux的设备驱动以及内核如何被模块化,在运行时插入和删除内核代码的动态机制.

	
	



		












	




