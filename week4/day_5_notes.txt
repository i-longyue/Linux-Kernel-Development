8.3.3 老的BH机制
	每个BH处理程序都严格地按照顺序执行----不允许任何两个BH处理程序同时执行,即使它们
	的类型不同,这样做倒是使同步变得简单了,可是却不利于任何两个BH程序同时执行,

	除了这些特点,BH机制和tasklet就很像了.实际上,在2.4内核中,BH就是基于tasklet实现的
	所有可能的32个BH都通过在<linux/interrupt.h>中定义的常量表示.如果需要将一个BH标志
	为挂起,可以把相应的BH号传给mark_bh()函数.在2.4内核中,这将导致随后调度BH tasklet.

	由于这种形式的下半机制存在缺点,内核开发者们希望引入任务队列机制来代替它,尽管任务
	队列得到了不少使用者的认可,但它实际上并没有达成这个目标.

8.4 工作队列
	工作队列是另外一种将工作推后执行的形式,它和我们前面讨论的所有其他形式都不相同.
	工作队列可以把工作推后,交由一个内核线程去执行--这个下半部分总是会在进程上下文中
	执行.这样,通过工作队列执行代码能占尽进程上下文的所有优势.最重要的就是工作队列
	允许重新调度甚至是睡眠.

	通常,在工作队列和软中断/tasklet中作出选择非常容易,如果推后执行的任务需要睡眠,
	那么就选择工作队列.如果推后执行的任务不需要睡眠,那么就选择软中断或tasklet.
	实际上,工作队列通常可以用内核线程替换.但是由于内核开发者们反对创建新的内核线程
	(在有些场合)

	如果你需要用一个可以重新调度的实体来执行你的下半部处理,你应该使用工作队列.它是
	唯一能在进程上下文件中运行的下半部实现机制,也只有它才可以睡眠.这意味着在你需要获
	得大量内存时,在你需要获取内核信号量时,在你需要执行阻塞式的







8.4.1 工作队列的实现
	工作队列子系统是一个用于创建内核线程的接口,通过它创建的进程负责执行由内核其他部分
	排到队列里的任务.它创建的这些地内核线程称作工作者线程.工作队列可以让你的驱动程序
	创建一个专门的工作者线路来处理需要推后的工作.

	工作者线程会从多个地方得到被推后的工作.许多内核驱动程序都把它们下半部分给工作者
	线程去做.除非一个驱动程序或者子系统必须建立一个属于它自己的内核线程,否则最好使用
	线程.

	1.表示线程的数据结构
	工作线程用workqueue_struct结构表示:
	struct workqueue_struct{

	}
	注意,每个工作者线程类型关联一个自己的workqueue_struct.在该结构里面,给每个线程
	分配一个cpu_workqueue_struct,因而也就是给每个处理器分配一个,因为每个处理器都有
	一个该类型的工作都线程

	2.表示工作的数据结构
	所有的工作者线程都是用普通的内核线程实现,它们都要执行worker_thread()函数.
	在它初始化以后,这个函数执行一个死循环并开始休眠.当有操作插入到队列里的时候,线程
	就会被唤醒,以便执行这些操作.当没有操作时,它又会继续休眠

	当一个工作者线程被唤醒时,它会执行它的链表上所有的工作.工作被执行完毕,它就将相应
	的work_struct对象从链表上移去.当链表上不再有对象的时候,它会继续休眠

	我们可以看一下worker_thread()函数的核心流程,简化如下:
	for(;;){
			prepare_to_wait(&cwq->more_work, &wait, TASK_INTERRUPTIBLE);
			if (list_empty(&cwq->worklist))
					schedule();

			finish_wait(&cwq->more_work, &wait);
			run_workqueue(cwq);
	}

	该函数在死循环中完成了以下功能
	1)线程将自己设置为休眠状态(state被设置成TASK_INTERRUPTIBLE),并把自己加入到等待
	队列中.
	2)如果工作链表是空的,线程调用schedule()函数进入睡眠状态.
	3)如果链表中有对象,线程就会睡眠,相反它将自己设置成TASK_RUNNING,脱离等待队列.
	4)如果链表非空,调用run_workqueue()函数执行被推后的工作.
	下一步,由run_workqueue()函数来实现完成推后到此的工作:

	while(!list_empty(&cwq->worklist)) {
		struct 


	}
	该函数循环遍历链表上每个待处理的工作,执行链表每个节点上的workqueue_struct中的func
	成员函数:
	1)当链表不为空时,选取下一个节点对象.
	2)获取我们希望执行的函数func及其参数data.
	3)把该节点从链表上解下来,等待处理标志位pending清零
	4)调用函数
	5)重复执行

3.工作队列实现机制的总结
 工作,工作队列,工作者线程之间的关系
	

 工作者线程<---------cpu_workqueue_struct         每个CPU有一个


					workqueue_struct结构体        每个工作者线程有一个
					 


					work_struct结构体             每个delay函数有一个

				
位于最高一层的是工作者线程.系统允许有多种类型的工作者线程存在.
对于指定的一个类型,系统的每个CPU上都有一个该类的工作者线程.内核中有些部分
可以根据需要要来创建工作者线程,而默认情况下内核只有event这一类工作者线程.
每个工作者线程都由一个cpu_workqueue_struct结构表示.而workqueue_struct结构
体则表示给定类型的所有工作者线程.


大部分驱动程序使用的现存的默认工作线程,它们使用起来简单方便,可是,在有些要求更严格
的情况下,驱动需要自己的工作者线程,比如说XFS文件系统就为自己创建了两种新的工作
者线程.

8.4.2 使用工作队列

1.创建推后的工作
首先要做的是实际创建一些需要推后完成的工作.可以通过DECLAR_WORK在编译时静态地建
该结构体:
	DECLAR_WORK(name, void(*func)(void *), void *data);

	这样就会静态地创建琴名为name,处理函数func,参数为data的work_struct结构体
	同样,也可以在运行时通过指针创建一个工作:

	INIT_WORK(struct work_struct *work, void(*func)(void *), void *data);
	这会动态地初始化一个由work指向的工作,处理函数为func,参数为data.

2.工作队列处理函数
void work_handler(void *data)


3.对工作进行调度
想要把给定工作的处理函数提交给events工作线程,只需调用;
	schedule_work(&work);
	work马上就会被调度,一旦其所在的处理器上的工作者线程被唤醒,它就会被执行.
	
	schedule_delayed_work(&work, delay);

4.刷新操作
	void flush_scheduled_work(void);
	该函数并不取消任何执行的工作.
	取消执行的工作应该调用
	int cancle_delayed_work(struct work_struct *work);
	这个函数可以取消任何与work_struct相关的挂起工作.

5.创建新的工作队列
创建一个新的任务队列和与之相关的工作线程,你只需要调用一个简单的函数:
struct workqueue_struct *create_workqueue(const char *name);

name参数用于该内核线程的命名.
struct workqueue_struct *keventd_wq;
keventd_wq = create_workqueue("envents");

这样就会创建所有的工作者线程,并且做好所有开始处理工作之前的准备工作
唯一的区别就在于它们针对给定的工作队列而不是events队列进行操作.
queue_work(struct workqueue_struct *wq, struct work_struct *work)
queue_delayed_work(struct workqueue_struct *wq,

		)

flush_workqueue(struct workqueue_struct *wq)

	

8.4.3 老的任务队列机制
任务队列机制通过定义一组队列来实现其功能.每个队列都有自己的名字,比如调度程序队列
立即队列和定时器队列.不同的队列在内核中的不同场合使用.keventd内核线程负责执行
调度程序队列的相关任务.

唯有调度队列有点意义,它能用来把工作推后到进程上下文完成.

8.5 下半部机制的选择
	在各种不同的下半部实现机制之间做出选择是很重要的.在当前的2.6版本内核中,有三种
可能的选择:软中断,tasklet和工作队列.工作队列机制与它们完全不同,它靠内核线程实现.
如果你需要把任务推后到进程上下文中完成,那么在这三者中就只能选择工作队列了.如果
进行上下文件并不是必须的条件,那么软中断和tasklet可以更合适

如果讲到易于使用,工作队列就当仁不让

8.6  在下半部分之间加锁
	到现在为止,我们还没讨论过锁机制,这是一个非常有趣.我们将在第9章和第10章仔细讨论它.
	不过,在这里还是应该对它的重要性有所了解:在使用下半部机制时,即使是在一个单处理器
的系统上,避免共享数据被同时访问也是至关重要的.一个下半部实际上可能在任何时候执行.
	使用tasklet的一个好处在于,它自己负责执行的序列化保障:两个相同类型的tasklet不允
	同时执行,即使在不同的处理器上也不行.这意味着你无须为intra-tasklet的同步问题操心
	了.tasklet之间的同步(就是使用两个不同类型的tasklet共享同一数据时)需要正确使用锁
	机制.
	
	如果进程上下文和一个下半部共享数据,在访问这些数据之前,你需要禁止下半部的处理
	并得到锁的使用权.做这些是为了本地和SMP的保护并且防止死锁出现.

	任何在工作队列中被共享的数据也需要使用锁机制.其中有关锁的要点和在一般内核代码
	中没什么区别,因为工作队列本来就是在进程上下文中执行的.

	会在第9章里,我们会揭示锁的奥妙.而在第10章中,我们将讲述内核的加锁原语.这两章
	会描述如何保护下半部使用的数据.

	8.7 禁止下半部
	一般单纯禁止下半部的处理是不够的.为了保证共享数据的安全,更常见的做法是,先
	得到一个锁然后再禁上下半部的处理.驱动程序中通常使用的都是这种方法,在第10章
	会详细介绍.然而,如果你编写的是内核的核心代码,你也只能仅需要禁止下半部就可以了.

	如果需要禁止所有的下半处理(明确点说,就是所有的软中断和所有地的tasklet),可以
	调用local_bh_diasble()函数.允许下半部处理,可以调用local_bh_enable()函数.没错
	这些函数的命令也有问题,可是你既然BH接口早就让位给软中断了,那么谁又会去改这些名称
	呢?
	下半部机制控制函数的清单
	函数                                         描述
	void local_bh_disable()                    禁止本地处理器的软中断和tasklet的处理
	void local_bh_enable()                     激活本地处理器的软中断和tasklet的处理
	这些函数有可能被嵌套使用----最后调用的local_bh_enable()最终激活下半部.比如第一次
	调用local_bh_disable(),则本地软件中断处理被禁止;如果local_bh_disable()被调用三次
	则本地处理仍然被禁止;只有当第四次调用local_bh_enable()时,软中断处理才被重新激活.
	
	函数通过preempt_count为每个进程维护一个计数器.当计数器变为0时,下半部才能够被处理.
	因为下半部的处理已经被禁止,所以local_bh_enable()还需要检查所有现存的待处理的下半
	部并执行它们.
	
void local_bh_disable(void)
{
	struct thread_info *t = current_thread_info();
	t->preempt_count	+= SOFTIRQ_OFFSET;
}

减少preempt_count如果该返回值为0,将导致自动激活下半部
执行挂起的下半部

void local_bh_enable(void)
{
	struct thread_info *t = current_thread_info()
	t->preempt_count -= SOFTIRQ_OFFSET;
	
	preempt_count是否为0,另外是否有挂起的下半部,如果都满足,则执行待执行的下半部
}
  这些函数并不能禁止工作队列的执行.因为工作队列是在进程上下文中运行的,不会涉及
  异步执行的问题,所以也就没有必要禁止它们执行.由于软中断和tasklet是异步发生的

8.8 小结
	在本章中,我们涵盖了用于Linux内核工作的三种机制:软中断,tasklet和工作队列.
	我们考察了其设计和实现,讨论了如何把这些机制应用到代码中,也调侃了易于混淆
	的命名,为了完整的起见,我们也考察了曾经的下半部机制:bh和任务队列---这些用在
	以前的Linux内核版本中

	因为下半部中相当程序地用到了同步和并发,所以本章谈了很多相关的话题,
	讨论了禁止下半部的问题,这是由并发保护引起的.


	第九章将从理论上讨论内核同步和并发,为理解这一问题的本质打下基础.第10章将
	讨论我们心爱的内核为解决这一问题提供的具体接口.以这两章为基,你的梦想就得以
	实现.














第9章 内核同步介绍
	2.6内核版本的出现,Linux内核已发展成抢占式内核,这意味着调度程序可以在任何
	时间抢占正在运行的内核代码,重新调度其他的进程执行.现在,内核代码中有不少部分都
	能够同步执行,而它们都必须妥善地保护起来.

	本章我们先提纲挈领式地讨论操作系统内核中并发和同步问题,第10章我们将详细介绍
	Linux内核为解决同步问题和防止产生竞争条件而提供的机制及接口.

9.1 临界区和竞争条件
	所谓临界区(也称为临界段)就是访问和操作共享数据的代码段.多个执行线程并发访问
	同一个资源通常是不安全的,为了避免在临界区中并发访问,编程者必须保证这些代码原子
	地执行----也就是说,操作在执行结束前不可被打断,就如同整个临界区是一个不可分割的
	指令一样.如果两个执行线程有可能处理同一个临界区中同时执行,那么这就是程序
	包含的一个bug.如果这种情况确实发生了,我们就称它是竞争条件,这样命令是因为
	这里会存在线程竞争.

	避免并发和防止竞争条件称为同步

9.1.1 为什么我们需要保护
	为了认清同步的必要性,我们首先要明白临界区无处不在.作为第一个例子


	int total = get_total_form_account()       /* 账号上的总额 */
	int withdrawal = get_withdrawal_amount()   /* 用户要求的取款额 */

	/* 检查用户账号上是否有足够的金额 */


	/* 好啦,用户有足够的金额,从总额操作取*/


9.1.2 单个变量
	现在,让我们看一个特殊计算的例子,考虑一个非常简单的共享资源,一个全局整形变量和一
	个简单的临界区,其中的操作仅仅是将整形变量的值增加1

	现在假定有两个执行线程同时进入这个临界区,如果i的初始值是7,那么,我们所期望的结果
	应该像下面这样(每一行代表一个时间单元):

		线程1           线程2

	两个原子操作交错执行根本就不可能发生,因为处理器会从物理上确保这种不可能.
	使用这样的指令会缓解这种问题,内核也提供了一组实现这些地原子操作的接口,我们
	将在第10章讨论它们.

9.2 加锁
	现在我们来讨论一个更为复杂的竞争条件,相应的解决方法也更为复杂.假设需要处理
	一个队列上的所有请求.我们假定该队列是通过链表得以为实现,链表中每个结点就代表
	一个请求.有两个函数可以用来操作此队列:

		一个函数将请求添加到队列尾部,另一个函数从队列头删除请求,然后处理它.
		内核各个部分都会调用这两个函数,所以内核会不断地在队列中加入请求,从
		队列中删除和处理请求.对请求队列的操作无疑要用到多条指令.如果一个线程试图
		读取队列,而这里正好另一个线程正在处理该队列,那么读取线程就会发现队列此时
		下处于不一致状态.很明显,如果允许并发访问队列,就会产生危害.当共享资源是一个
		复杂数据结构时,竞争条件往往会使用该数据结构破坏.


		表面上年看这种情况好像没有一个好的方法来解决,一个处理器读取队列的时候,我们
		怎么能禁止另一个处理器更新队列呢?虽然有些体系结构提供了简单的原子指令.
		实现算术运算和比较之类的原子操作,但体系结构提供专门的指令,对像上例中那样的
		不定长度的临界区进行保护,我们需要一种方法提供确保有且只有一个线程对数据结构
		进行操作,或者当另一个线程在对临界区标记时,就禁其他访问.

	
锁提供的就是这种机制:它就如同一把门锁,门后的房间可想象成一个临界区.在一个指定时间内,
房间内,只能有一个执行线程存在,当一个线程进入房间后,它会锁住后的房间;当它结束对共享
数据的操作后,就会走出房间,打开门锁.如果另一个线程在房门上锁时来了,那么它就必须等待
房间内的线程出来并打开门锁,才能进入房间.


	前面例子中讲到的请求队列,可以使用一个单独的锁进行保护.每当有一个新请求要加入
	队列,线程会首先占住锁,然后就可以安全地将请求加入到队列中,结束操作后再释放该锁
	同样当一个线程想从请求队列中删除一个请求时,也需要先占住锁,然后才能从队列中读取
	和删除请求,而且在完成操作后也必须释放锁.任何要访问队列的其他线程也类似,必须占住
	锁后才能进行操作

	任何要访问队列的代码首先都需要占住相应的锁,这样该锁就能阻止来自执行线程的并发
	访问:
	线程1            线程2
	试图锁定队列    试图锁定队列
	成功:获得锁     fail
	访问队列..      wait
	为队列解除锁    wait
					成功:获得锁	
	...             访问队列...
                    为队列解除锁
	请注意锁的使用是自愿的,非强制的,它完全属于一种编程者自选的编程手段.
	没有什么可以强制编程者在操作我们虚构的队列时必须使用锁.当然,如果不这么作,无疑
	会有竞争条件而破坏队列.
		
	锁有多种多样的形式,而且加锁的粒度范围也各不相同
	Linux实现了几种不同的锁机制.各种锁机制之前的区别主要在于:
	当锁已经被其他线程持有,因而不可用时的行为表现,一些锁被争用时会简单地执行忙等待,
	而另外一些锁会使用当前任务睡眠直到锁可用为止.

	第10章我们将讨论Linux中不同锁之间的行为差别及它们的接口.
	在流行的x86体系结构中,锁的实现也不例外,它使用了称为compare和exchange的类似指令.


9.2.1 造成并发执行的原因
	用户空间之所以需要同步,是因为用户程序会被调度程序抢占和重新调度,由于用户
	进程可能在任何时间被抢占,而调度程序完全可能选择另一个高优先级的进程到处理器上
	执行,所以就会使用一个程序正处于临界区,被非自愿抢占了,如果新调度的进程随后也进入
	同一个临界区(比如说,这两个进程要操作共享的内存,或者向一个文件描述符中写入),前
	后两个进程相互之间就会产生竞争.另外因为信号处理是异步发生的,所以即使是单线程的
	多个进程共享文件,或者在一个程序内部处理信号,也有可能产生竞争条件.这种类型的并发
	操作


	如果你有一台支持对称多处理器的机器,那么两个进程就可以真正地在临界区中同时执行了
	,这种类型称为真并发.

	但它们都同样会造成竞争条件,而且也需要同样的保护.
	内核中有类似可能造成并发执行的原因,它们是:
	*中断----中断几乎可以在任何时刻异步发生
	*软中断和tasklet----内核能在任何时刻唤醒调度软中断和tasklet,打断当前正在执行的
	代码

	*内核抢占
	*睡眠及用户空间的同步
	*对称多处理
	对内核开发者来说,必须理解上述这些并发执行的原因,并且为它们事先做好准备工作.
	如果在一段内核代码操作某资源的时候系统产生了一个中断,而且该中断的处理程还要
	访问还要访问这个资源,这就是一个bug

	还要注意,两个处理器绝对不能同时访问同一个共享数据.当我们清楚什么样的数据需要
	保护时,提供锁来保护系统稳定也就不难做到了,然而,真正困难的就是发现上述的潜在并发
	执行的可能,并有意识地来防止并发执行.

	我们要重申这点,因为它实在是很重要的.其实,真正用锁来保护共享资源并不困难,真正
	需要共享的数据和相应的临界区,才是真正有挑战的地方.要记住,最开始最开始设计代码
	的时候就要考虑加入锁,而不是事后才想到,如果代码已经写好了,再在其中找到需要上锁
	的部分并向其中加锁是非常困难的.

	在中断处理程序中能避免并发访问的安全代码称作中断安全代码,
	在对多处理器中避免并发访问的安全代码称为SMP安全代码
	在内核抢占时能避免并发访问的安全代码称为抢占安全代码
	
	在第10章会重点讲述了为提供同步和避免所有上述竞争条件,内核所使用的实际方法.


9.2.2 了解要保护些什么
	找出哪些数据需要保护是关键所在.由于任何可能并发访问的代码都几乎无例外地需
	要保护,所以寻找哪些代码不需要保护反而相对更容易些

	执行线程的局部数据仅仅被它本身访问,显然不需要保护,比如,局部自动变量(还有动态
			分配的数据结构,其地址仅存放在堆栈中)不需要任何形式的锁,因为它们独立于

	执行线程的栈中.类似的,如果数据只会被特定的进程访问,那么也不需要加锁

	到底什么数据需要加锁?大多数内核数据结构都需要加锁!有一条很好经验可以帮助我们
	判断:如果有其他执行线程可以访问这些数据,那么就加锁,如果任何其他什么东西都能
	看到它,那么就要锁住它.记住:要给数据而不是代码加锁


	SMP UP
		因为Linux内核可在编译时配置
		这样做可以使单处理器机器避免使用自旋锁带来的开销.
		实际编译时包含的锁就不同

		在代码中,要为大多数情况提供适当的保护,例如具有内核抢占的SMP,并且要考虑
		到所有的情况

	在编写内核代码时,你要问自己下面这些问题:

*这个数据是不是全局的?除了当前线程外,其他线程能不能访问它?
*这个数据会不会在进程上下文件和中断上下文中共享?它是不是要在两个不同的中断处理
	程序中共享?
	
具体加锁方法将在第10章进行讨论.


9.3 死锁
	死锁的产生需要一定条件:要有一个或多个执行线程和一个或多个资源,每个线程都在等待
	其中一个资源,但所有的资源已经被占用了.所有线程都相互等待,但它们不会释放已经占有
	的资源.于是任何线程都无继续

9.4 争用和扩展性


第10章
内核中同步方法
与原子操作中整数不同,代码一般无法选择是否使用位操作,它们是唯一的,具有可以移植性
的设置特定位方法,需要选择是使用原子位操作还是非原子位操作,通常这样执行












	

10.4 信号量 Linux中的信号量是一种睡眠锁.如果有一个任务试图获得一个不可用(已经被占用)的信号量

