4.4.3 公平调度
  CFS的出发点是基于一个简单的理念：进程调度的效果应该如同系统具备一个理想中的完美多任务处理器。
在这种系统中，每个进程将能获得1/n的处理器时间----n是指可运行进程的数量。
同时，我们可以调度给它们无限小的时间周期，所以在任何可测量周期内，我们给予n个进程中每个进程同样
多的运行时间。举例来说，假如我们有两个进程，在标准unix调度模型中，我们先运行其中一个5ms,然后再
运行一个5ms，然后再运行一个5ms。但它们任何一个运行时，都将占有100%的处理器。而在理想情况下，完美的
多任务处理器模型应该是这样的：我们能在10ms内同时运行两个进程，它们各自使用处理器一半的能力。
当然，上述理想模型并非现实,因为我们无法在一个处理器上真的同时处理多个进程。
而且如果每个进程运行无限小的时间周期也是不高效的--因为调度时进程抢占会带来一定的代价：
将一个进程换出，另一个换入本身有消耗，同时还会影响到缓存的效率。
因此虽然我们希望所有的进程只能运行一个非常短的周期，但是CFS充分考虑了这将带来的额外消耗
实现中首先要确保系统性能不受损失。CFS的做法是允许每个进程运行一段时间，循环轮转，选择运行
最少的进程作为下一个运行进程，而不再采用分配给每个进程时间片的做法了。CFS在所有可运行进程总
数基础上计算出一个运行进程，而不再采用分配给每个进程时间片的做法了，CFS在所有可运行进程总数
基础上计算出一个进程应该运行多久，而不是依靠nice值来计算时间片。nice值在CFS中被作为进程获得
的处理器运行比的权重。越高的nice值（越低的优先级）进程获得更低的处理器使用权重，这是相对默认
nice值进程而言的;相反，更低的nice值（越高的优先级）的进程获得更高的处理器使用权重。
	每个进程都按其权重在全部可运行进程中所占比例的“时间片"来运行。为了计算准确的时间片，CFS
为完美多任务中无限小调度周期的近似值设立了一个目标。而这个目标称作”目标延迟“，越小的调度
周期将带来越好的交互性，同时也更接近完美的多任务。但是你必须承受更高的切换代价和更差的系统的
系统吞吐能力。
	???让我们假定目标延迟值是20ms，我们有两个同样优先级的可运行任务（无论这些任务的优先级是多个）
每个任务在被其它任务抢占前运行10ms，如果我们有4个这样的任务，则每个只能运行5ms。进一步设想，
如果有20个这样的任务，那么每个仅仅只能获得1ms的运行时间。

	你一定注意到了，当可以运行任务数量趋于无限时，它们各自获得的处理使用比和时间片
	都将趋于0。这样无疑造成了不可接受的切换消耗。CFS为此引入每个进程获得的时间片底线，
	这个底线称为最小粒度。默认情况下这个值是1ms。如此一来，即使是可运行进程数量趋于无穷，
	每个最少也能获得1ms的运行时间，确保切换消耗被限制在一定范围内。(敏锐的读者会注意到
	假如进程数量变得非常多的情况下，CFS并非一个完美的公平调度。因为这时处理器时间片再小
	也无法突破最小粒度。的确如此，尽管修改过的公平队列方法确实能提高这方面的公平性，但
	是CFS的算法本身其实已经决定在这方面做出折中了。但还好，因为通常情况下系统中只会有几百个
	可运行进程，无疑，这时，CFS是相当公平的。)

	现在，让我们再来看看具有不同nice值
	的两个可运行进程的运行情况----比如一个具有默认nice值（0），另一个具有的nice值是5。这些
	不同的nice值对应不同的权重，所以上述两个进程将获得不同的处理器使用比。在这个例子中，

	nice值是5的进程的权重将是默认nice进程1/3
	???什么是目标延迟
	.如果我们的目标延迟为20ms，那么这两个进程值对应不同的权重，所以上述两个进程将获得不同
	处理器使用比。在这个例子中，nice值是5的进程。绝对的nice值不再影响调度决策：只有相对值才
	会影响处理器时间分配比例。

	总结一下，任何进程所获得的处理器时间是由它和其他所有可运行进程nice值的相对差值决定的。
	nice值对时间片的作用不再是算数加权，而是几何加权。任何nice值对应的绝对时间不再是一个
	绝对值，而是处理器的使用比。CFS称为公平调度器是因为它确保给每个进程公平的处理器使用比。
	正如我们知道的，CFS不是完美的公平，它只是近乎完美的多任务。但是它确实在多进程环境下，
	降低了调度延迟带来的不公平性。


4.5 Linux调度的实现
	在讨论了采用CFS调度算法的动机和其内部逻辑后，我们现在可以开始具体探索CFS是如何实现的。
	其相关代码位于文件kernel_sched_fair.c中,我们将特别关注其四个组成部分：
	*时间记账
	*进程选择
	*调度器入口
	*睡眠和唤醒


4.5.1 时间记账
	所有的调度器都必须对进程时间做记账。多数Unix系统正如我们所说，分配一个时间片给每一个进程。
	那么当每次系统时间节拍发生时，时间片都会被减少一个节拍周期。当一个进程的时间片被减少到0时，
	它就会被另一个尚未减到0的时间片可运行进程抢占。

	1.调度器实体结构
	CFS不再有时间片概念，但是它也必须维护每个进程运行的时间记账，因为它需要确保每个进程只
	在公平分配给它的处理器时间内运行。
	CFS使用调度器实体结构(定义在文件<linux/sched.h>的struct_sched_entity中)来追踪进程运行
	记账:
	调度器实体结构作为一个名为se的成员变量，嵌入在进程描述符struct
	task_struct内。
	struct sched_entity{
		struct load_weight load;
		struct rb_node	   run_node;
		struct list_head   group_node;
		unsigned int       on_rq;
		u64                exec_start;
		u64                sum_exec_runtime;
		u64                vruntime;
		u64                prev_sum_exec_runtime;
		u64                list_wakeup;
		u64                avg_overlap;
		u64                nr_migrations;
		u64                start_runtime;
		u64                avg_vakeup;
		/* 这里省略了很多统计变量，只有在设置了CONFIG_SCHEDSTATS时才启用这些变量 */
	}

	2.虚拟实时
	vruntime变量存放进程的虚拟运行时间，该运行时间（花在运行上的时间和）的计算是经过
	可运行进程总数的标准化（或者说是被加权的）。虚拟时间是以ns为单位的，所以vruntime
	和定时器节拍不再相关。虚拟运行时间可以帮助我们逼近CFS模型所追求的”理想多任务处理“
	如果我们真有这样一个理想的处理器，那么我们就不再需要vruntime了。
	因为优先级相同的所有进程的虚拟运行时间都是相同的--所有任务都将收到相等的处理器份额。
	但是因为处理器无法实现完美的多任务，它必须依次运行每个任务。因此CFS使用vruntime变量
	来记录一个程序到底运行了多长时间以及它还应该再运行多久。
	定义在kernel/sched_fair.c文件中的update_curr()函数实现了该记账功能:
	static void update_curr(struct cfs_rq *csf_rq)
	{

	

	}

	update_curr()计算了当前进程的执行时间，并且将其存放在变量delta_exec中。然后它又将
	运行时间传递给了
	__update_curr(),由后者再根据当前可运行进程总数对运行时间进行加权计算。最终将上述的
	权重值与当前运行进程的vruntime相加。
	update_curr()是由系统定时器周期性调用的，无论是在进程处于可运行态，还是被堵塞处于
	不可运行态。根据这种方式，vruntime可以准确地测量给定进程的运行时间，而且可知道谁该
	是下一个被运行的进程。

	4.5.2 进程选择
		在前面内容中我们的讨论中谈到若存在一个完美的多任务处理器，所有可运行进程的
	vruntime值将一致。但事实上我们没有找到完美的多任务处理器，因此CFS试图利用一个简单
	的规则去均衡进程的虚拟运行时间：
	
	当CFS需要选择下一个运行进程时，它会挑一个具有最小vruntime的进程。这其实就是CFS调度
	算法的核心：选择具有最小vruntime的任务。那么剩下的内容我们就来讨论到底如何实现选择
	具有最小vruntime值的进程。

		CFS使用红黑树来组织可运行进程队列，并利用其迅速找到最小vruntime值的进程。在linux
	中，红黑树称为rbtree,它是一个自平衡二叉搜索树。我们将在第5章讨论自平衡二叉树以及红黑
	树。红黑树是一种以树节点形式存储的数据，这些数据都会对应一个键值。我们可以通过这些
	键值来快速检索节点上的数据(重要的是，通过键值检索到的对应节点的速度与整个树的节点
	规模成指数比关系)。

	1.挑选下一个任务
	我们先假设，有那么一个红黑树存储了系统中所有的可运行进程，其中节点的键值是可运进程的
	虚拟运行时间。稍后我们可以看到如何生成树，但是我们假定已经拥有它了。CFS调度器选取
	待运行的下一个进程，是所有进程中vruntime最小的那个它对应的便是在树中最左侧的叶子节点。


	也就是说，你从树的根节点沿着左边的子节点向下找，一直找到叶子节点，你便找到了基vruntime
	值最小的那个进程。
	(再说一次，如果你不熟悉二叉搜索树，不用担心，只要知道它用来加速寻找过程即可)
	CFS的进程选择算法可简单总结为”运行rbtree树中最左边叶子节点所代表的那个进程“
	实现这一过程的函数是__pick_next_entity(),它定义在文件kernel/sched_fair.c中
	static struct sched_entity *__pick_next_entity(struct cfs_rq *cfs_rq)
	{
		struct rb_node *left = cfs_rq->rb_leftmost;
		if(!left)
			return NULL;
		return rb_entry(left, struct...);
	}


	2.向树中加入进程
	现在，我们来看CFS如何将进程加入rbtree中，以及如何缓存最左叶子节点。这一切发生在进程变
	为可以运行状态（被唤醒）或者是通过fork()调用第一次创建进程时----在第三章我们讨论过它。
	enqueue_entity()函数实现这一目的:


	static void
	enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
	{
		/* 通过调用update_curr(),在更新 */

	}
	该函数更新运行时间和其他一些统计数据，然后调用__enqueue_entity进行繁重的插入动作，把
	数据项真正插入到红黑树中：
	
	我们来看看上述函数，while()循环中遍历树以寻找合适的匹配键值，该值就是被插入进程的
	vruntime。平衡二叉树的基本规则是，如果键值小于当前节点的键值，则需要转向树的左分支
	相反如果大于当前节点的键值，则转向右分支。如果一旦走过右边分支，哪怕一次。也说明
	插入的进程不会是新的最左节点，因此可以设置leftmost为0。如果一直都是向左移动，那么
	leftmost维持1，说明我们有一个新的最左节点，并且可以更新缓存----设置rb_leftmost指向
	被插入的进程。

	当我们沿着一个方向和一个没有子节点比较后：link如果这时是NULL，循环随之终止。当退出
	循环后，接着在爷节点上调用rb_link_node(),以使得新插入的进程成为其子节点。最后函数
	rb_insert_color()更新树的自平衡相关属性。)

	3.从树中删除进程
	最后我们看看CFS是如何从红黑树中删除进程的。删除动作发生在进程堵塞(变为不可运行态)
	或者终止时
	static void
	dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int sleep))
	{

	}
	和给红黑树添加进程一样，实际工作是由辅助函数__dequeue_entity()完成的。
	从红黑树中删除进程要容易很多。因为rbtree实现了rb_erase()函数,它完成所有工作。该函数
	的剩下工作是更新rb_leftmost缓存。如果要删除的进程是最左节点，那么该函数要调用
	rb_next()按顺序遍历，找到谁是下一个节点，也就是当前最左节点被删除后，新的最左节点。


	4.5.3 调度器入口

		进程调度的主要入口点是函数schedule(),它定义在文件kernel/sched.c中。
		它正是内核其他部分用于调用进程调度器的入口：
		选择哪个进程可以运行，何时将其投入运行。shedule()通常都需要一个具体的调度类关联

		也就是说，它会找到一个最高优先级的调度类----后者需要有自已的可运行队列，然后问
		后者谁才是下一个该运行的进程。知道了这个背景，就不会吃惊schedule()函数为何实现
		得如此简单。该函数中唯一重要的事件是，它会调用pick_next_task
		pick_next_task会以优先级为序，从高到低，依次检查第一个调度类，并且从最高优先级
		的调度类中，选择最高优先级的进程：

		static inline struct task_struct *pick_next_task(struct rq *rq)

	注意该函数开始部分。因为CFS是普通进程的调度类，而类型运行的绝大多数进程都是变通进程。
	因此有一个小技巧用来加速选择下一个CFS提供的进程。前提是所有可运行进程数量等于CFS
	类对应的可运行进程数
		该函数的核心是for()循环，它以优先级为序，从最高的优先级开始，遍历了每一个调度类。
	每一个调度类都实现了pick_next_task()函数，它会返回指向下一个可运行进程的指针。CFS中
	pick_next_task()实现会调用pick_next_entity,而该函数会再来调用我们前面内容中讨论的
	__pick_next_entity()函数。 


4.5.4 睡眠和唤醒
	休眠(被阻塞)的进程处于一个特殊的不可执行状态。这点非常重要，如果没有这种特殊状态的话，
	调度程序就可能选出一个本不愿意被执行的进程，更糟糕的是，休眠就必须以轮询的方式实现了。
	进程休眠有多种原因，但肯定都是为了等待一些事件。事件可能是一段时间从文件I/O读更多的
	数据，或者是某个硬件事件。一个进程还有可能在尝试获取一个已被占用的内核信号量时，被迫
	进入休眠。休眠的一个常见原因就是文件I/O----如进程对一个文件执行了read()操作，而这需要
	从磁盘里读取。还有，进程在获取键盘输入的时候也需要等待。无论哪种情况，内核的操作都相同
	进程把自已标记成休眠状态，从可执行红黑树中移出，放入等待队列，然后调用schedule()选择
	和执行一个其他进程。唤醒的过程刚好相反：进程被设置为可执行状态，然后再从等待队列中
	移动可执行的红黑树中。
	在第3章我们，休眠有两种状态，task_interruptible和task_uninteruptible。
	它们唯一的区别是处于task_uninterruptible的进程会忽略信号，而处于task_interruptible状态
	的进程如果接收到一个信号，会被提前唤醒并响应该信号。


	两种状态的进程位于同一一个等待队列上，等待某些事件，不能够运行。

	1等待队列
	休眠通过等待队列进行处理。等待队列是由某些事件发生的进程组成的简单链表。内核用
	wake_queue_head_t来代表等待队列。等待队列可以通过declare_waitqueue()
	静态创建，也可以由init_waitqueue_head()动态创建。进程把自已放入等待队列中并设置成不
	可执行状态。当与等待队列相关的事件发生的时候，队列上的进程会被唤醒。为了避免产生竞争
	条件，休眠和唤醒的实现不能有纰漏.

		针对休眠。以前曾使用过一简单的接口。但是那些接口会带来竞争条件：有可能导致在判定
		条件后变为真，进程却开始了休眠，那样就会使进程无限地休眠下去。所以在内核
	中进行休眠的推荐注是相对复杂一些：

	进程通过执行下面几个步骤将自已加入到一个等待队列中：
	1)调用宏DEFINE_wAIT()创建一个等待队列的项目
	2)调用add_wait_queue()把自已加入到队列中。该队列会在进程等待的条件满足时唤醒它。
	当我们必须在其他地方撰写相关代码。在事件发生时，对等待队列执行的wake_up()操作。
	3)prepare_to_wait()方法将进程的状态变量为TASK_INTERRUPTIBLE或者TASK_UNITERRUPTIBLE.
	而且该函数如果有必要的话会将进程加回到等待队列，这是在接下来的循环遍历中所需要的。

	4）如果状态被设置为TASK_INTERRUPTIBLE,则信号唤醒进程。
	5）当进程被唤醒的时候，它会再次检查条件是否为真，如果是，它就退出循环。如果不是
	它再次调用schedule()并一直重复这步操作。

	6)当条件满足后，进程将自已设置为TASK_RUNNING并调用finish_wait()方法把自已移出队列。
	如果在进程开始休眠后，进程就将自已设置为TASK_RUNNING并调用finish_wait()方法把自已
	移出等待队列。

		如果在进程开始休眠之前条件就已经达成了。那么循环回退出，进程不会存在错误地进入
	休眠的倾向。需要注意的是，内核代码在循环体内常常需要完成一些其他的任务，比如，它
	可能在调用schedule()之前需要释放掉锁，而在这以后再重新获取它们，或者响应其他事件。

	函数inotify_read(),负责从通知文件描述符中读取信息，它的实现无疑是等待队列的一个典型
	用法：

	static ssize_t inotify_read(struct file *file, char __user *buf,
			size_t count, loff_t *pos)
	{}
	这个函数的遵循了我们例子中的使用模式，主要是区别它在while循环中检查了状态，而不是在
	while循环条件语句中。原因是该条件的检测更复杂些，而且需要获得锁。也正因为如此，循环
	退出是通过break完成的。

	2.唤醒
	唤醒操作通过函数wake_up()进行，它会唤醒指定等待队列上的所有进程.它调用函数try_to_wake_
	up(),该函数负责将进程设置为TASK_RUNNING状态，调用enqueue_task()将此进程放入红黑树中，
	如果被唤醒的进程优先级比当前正在执行的进程的优先级高，还要设置need_resched标志。
	通常哪段代码促使促使等待条件达成，它就要负责附后调用wake_up()函数。
	举例来说，当磁盘数据到来时，VFS就要负责对等待队列调用wake_up(),以便唤醒队列中等待
	这些数据的进程。

	关于休眠有一点需要注意，存在虚假的唤醒。有时候进程被唤醒并不是因为它所等待的条件
	达成了才需要用一个循环处理来保证它等待的条件真正达成。


	4.6 抢占和上下文切换（进程切换)
	上下文切换，也就是从一个可执行进程切换到另一个进程，由定义在kernel/sched.c中的
	context_switch()函数负责处理。每当一个新进程被选出来准备投入运行的时候，scheddule()
	就会调用该函数。它完成两项基本的工作：
	
		*调用声明在<asm/mmu_context.h>中的switch_mm(),该函数负责把虚拟内存从上一个进程
		映射到新进程中。

	*调用声明在<asm/system.h>中的switch_to(),该函数负责从上一个进程的处理器状态切换到新
	进程的处理器状态。这包括保存，恢复栈信息和寄存器信息，还有其他任何与体系结构相关的
	状态信息，都必须以每个进程为对象进行管理和保存。

		内核必须知道在什么时候调用schedule().如果紧靠用户程序代码显式的调用schedule(),它
	们可能就会永远地执行下去。相反内核提供了一个need_resched标志来表明是否需要重新执行
	一次调度。
	当某个进程应该被抢占时，scheduler_tick()就会设置这个标志：
	每个进程都包含一个need_resched标志，这是因为访问进程描述符的数据要比访问一个全局变量
	快(因为current宏速度很快并且描述符通常都在高速缓存中)在2.2以前的内核版本中。而在2.6
	版中，它被移到thread_info结构体中，用一个特别的标志 变量中的一位来表示。
	


	4.6.2 用户抢占
	内核即将返回用户空间的时候，如果need_resched标志被设置，会导致schedule()被调用，此时
	就会就会发生用户抢占。在内核返回用户空间的时候，它知道自已是安全的，因为既然它可以继续
	去执行当前进程，那么它当然可以再去选择一个新的进程去执行。所以，内核无论是在中断处理
	程序还是在系统调用后返回，都会检查need_resched标志。如果它被设置了，那么，内核会选择一
	个其他(更合适的)进程投入运行。从中断处理程序或者系统调用返回路径都是跟体系结构相关的。
	有entry.s(此文件不仅包含内核入口部分的程序，内核退出部分的相关代码也在其中)文件中通过
	汇编语言来实现。
	简而言之，用户抢占在以下情况时，产生：
	从系统调返回用户空间。
	从中断处理程序返回用户空间时。



	4.6.2 内核抢占
		与其他大部分的Unix变体和其他大部分的操作系统不同，Linux完整地支持内核抢占。在不支持
		内核抢占的内核中，内核代码可以一直执行，到它完成为止。到它完成为止。也就是说：

		调度程序没有办法在一个内核级的任务正在执行的时候重新调度----内核中的各任务是以协作方式
		调度的，不具备抢占性。内核代码一直要执行到完成(返回用户空间)或明显的阻塞为止。在2.6版的
		内核中，内核引入了抢占内力。现在，只要重新调度是安全的，内核就可以在任何时间抢占正在执行
		的任务。

		那么，什么时候重新调度才是安全的呢？只要没有持有锁,内核就可以进行抢占。锁是非抢占区域
		的标志。由于内核是支持SMP的，所以如果没有持有锁，正在执行的代码就是可以重新导入的，也
		就是可以抢占的。

		为了支持内核抢占所做的第一处变动，就是为了每个进程的thread_info引入preempt_count计数器
		该计数器初始值为0.第当使用锁的时候加1，释放的进修减1.当数值为0的时候。内核就可以执行
		抢占。

		从中断返回内核空间的时候，内核会检查need_resched和preempt_count的值。如果need_resched被
		设置，并且preempt_count为0的话，这说明有一个更为重要任务需要执行并且可以安全地抢占。
		此时，调度程序就会被调用。如果preempt_count不为0，
		说明当前任务持有锁，所以抢占是不安全的
		这里，内核就会像通常那样直接从中断返回当前执行进程。如果当前进程持有的所有的锁被释放了，
		preempt_count就会重新为0.此时，释放锁的代码会检查need_resched是否被设置。如果是的话，
		就会调用调度程序。有些内核代码需要允许或禁止内核抢占，相关内容在第9章讨论。

		如果内核中的进程被阻塞了，或它显式的调用了schedule()，内核抢占也会显式地发生。这种形式
		的内核抢占从来都是受支持的。因为根本无须额外的逻辑来保证内核可以安全地被抢占。如果代码
		显式地调用了schedule(),那么它应该清楚自己是可以安全地被抢占。

		内核抢占会发生在：
		*中断处理程序正在执行，且返回内核内核之前。
		*内核代码再一次具有可抢占性的时候。
		*如果内核中的任务显式地调用schedule()
		*如果内核中的任务阻塞(这同样也会导致调用schedule())


	4.7 实时调度策略
		Linux提供了两种实时调度策略：SCHED_FIFO和SCHED_RR。而普通的，非实时的调度策略是SCHED_
		NORMAL。
		借肋调度类的框架，这些实时策略并不被完全公平调度器来管理
		而是被一个特殊的实时调度哭管理。
		我们讨论实时调度策略和算法。

		sched_FIFO实现了一种简单的先入先出的调度算法：它不使用时间片。处于可运行状态的SCHED_FIFO
		级进程会任何的SCHED_NORMAL级的进程都先得到调度。一旦一个SCHED_FIFO级处于可执行状态。就会
		一直执行，直到它自己受阻塞或显式地释放处理器为止;它不基于时间片。
		只有更高优先级的FIFO或者RR才能抢占。

		RR与FIFO大体相同，只是SCHED_RR级的进程耗尽事先分配给它的时间后就不能再继续执行了。也
		就是说，SCHED_RR是带时间片的SCHED_FIFO


	4.8 与调度相关的系统调用
		Linux提供了一个系统调用族，用于管理与调用程序相关的参数。这些系统调用可以用来操作和处理
		进程的优先级。


	4.8.1 与调度策略和优先
		









